
#checklist for running as of 2.19.16:
1. Set synth_data to true if running synth data
2. If running real data:
    a. Check the graph_idx. Is it what you want? (exictatory cells, etc)
    b. check the edge index. Is it what you want? (trim_50kb_w_self_edge_index, etc)
    c. check num_excitatory cells. Is it what you want? (len(graph_idx_list))
3. if running synth data
    a. check synth_data_stamp_name is for the dataset you want.
4. set hpc_run to true if running on hpc
5. set num_epochs
6. set batch_size


#version notes
#v0.2.1 you're going to load only the first 10 of the excitatory cells in the dataset class. 
#v0.3.1 added sag pool, extracted summed the sagpool scores to scoresum, added model save function with timestamp
#v0.4.1 added custom sag pool. See week 12 work notes for how to use. key feature: it returns an extra tensor with the complete sagpool scores.
#problem: pre_drop_score_sum collects scores from all epochs during trainging. We really only want the scores from the last epoch.
#v0.4.2 added total run time measurer
#0.5.2 testing last-epoch only score collector
#0.5.3 changed epoch runtime 
#0.6.3 added the scoresum collector
#0.6.4 fixed scoresum failing when run batch size != set batch_size (ie because num graphs not evenly divisible by batch_size)
#0.6.5 fixed epoch counter bug
#0.7.5 extracted scores with metadata and saved to csv
#0.7.5 fixed tensor size alignment issue with scoresum extraction tensor.size [n] != size [1,n]
#1.7.5 FULL RUN COMPLETE!
#1.8.5 added call rate counter, included it in model results, included weighted score in model results, renamed model results csv. Saved the 
#2.8.5 added two more convolution layers, two normalization layers, a dropout layer, and a second pooling layer. also softmaxxed the attention scores and took their product to output as the total graph attention score. 
#2.9.5 added a notes feature
#2.9.6 fixed misshape feed into sagpool caused by 2x attention heads and 2x hidden layer
#2.10.6 activated all changes from 1.8.5 to 2.9.6.
#2.11.6 added precision and f1 metrics. PROBLEM: IT ALWAYS PREDICTS THE SAME CLASS, sometimes 1, sometimes 0.
#2.11.7 changed adam lr to .0001 from .01.
#2.12.7 added the to.device() to accommodate gpu usage on hpc. model still runs on laptop
#2.12.9 changed a bunch of slashes around to keep the hpc happy
#2.12.11 more hpc grammar tweaks. script can't find pts/noedge_idx_0.pt
#2.13.11 added device checker that's called for main model only and goes to model_notes. 
#2.13.12 global variable for model_notes in the check_device function.
#2.13.13 more signposting print statements for hpc
#2.14.13 attempted to fix non-gpu tensor error by sending errant tensors to gpu and moving the todevice function to the top
#2.15.13 fixed send to device error on line 208 in the node call rate calculator. 
#2.16.13 added even more send to device errors
#2.17.13 now you have to send all the outputs to cpu before you can save them.
#2.18.14 added a parallelizer using nn.DataParallel. hopefully this will prevent crashing with 1 gpu. update: I stopped it early, but 99% confident it wont
#2.19.15 manually set num_classes = 2 instead of using the dataset.num_classes in the lin layer. This method loads all data objects to gpu, resulting in gpu death. 
#2.19.16 fixed batch size to 100, added print statements in more places, hopefully this will show up in o file
#2.19.21 added a first pass of archetecture for synth data. running test now. 
2.19.22 solved some issues with non-sparse synth data
2.23.22 finally hit the directory name size limit. moved everything to C:/Users/username/OneDrive - Imperial College London/0_Imperial_main_asof_1.19.23/0Thesis_Project/0MAIN
3.24.22 Synth works! however, the model can't really distinguish between 
3.25.22 sucks on synth data 1 gene above mean sets outcome. radically simplifying model to 1 conv and one sagpool followed by meanpool
3.26.22 added timestamp to model notes-- needed to connect job number on model runs to written output. 
3.27.22 removed the step printing. 
3.28.22 made learning rate a parameter and set from .0001 to .01
3.30.22 added metrics by epoch csv output and predropscores by epoch csv output. graph them to see how ht emodel learns over time. 
3.31.22 fixed call rate with category vector in synth data. note: call rate for real data with category vector will still be wrong. 

#testing notes
#t1: 10 cells loaded in dataset class with 50kb edge index w self loops. 
# Dataset definition bottleneck solved. You shouldn't include raw file names you can't use. 
# Performance is poor: only .4 acc on the test set.
#t2: v0.5.2, 100 cells loaded w 50kb self loop edge index
# runtime: ~5m per epoch, terminated after two epochs
# performance on two epochs: train acc: .6 test acc: .3
#t3: v1.8.5. 600s/epoch, 20ish mins(?) of class instantiation time
#t4: v1.8.5 all excitatory cells, left to run overnight. failed because of memory overallocation
#t5: v1.8.5 tried 20000 excitatory cells. failed because of memory overallocation. always step 1 of forward pass
#t6: v1.8.5 tried 10000 excitatory cells. failed because of memory allocation.
#t7: v1.8.5 tried 5000 excitatory cells. failed mem alloc. I think it's the batch size of 100, not the cell count. 
#t8: v1.8.5 tried 1000 exctatory cells, batch soze = 10. ran fine. total time 14k sec. test acc .63. scores ar weird-- top 1000 all sam score. 
#t9: v1.8.5 tried all excitatory cells, batch size = 10. 
#t10: v2.10.6 tried 1000 excitatory cells, batch size = 10. computer froze.
#t11: v2.10.6 rerun of t10
#t12: v2.11.6 run of 100 cells, batch = 10. need some data for the visualizer. cpu crash. lowering batchs size to 2. 
#t12: v2.11.6 run of 100 cells, batch = 2. no crash. still pred all the same values. 
#t12: v2.11.7 run of 1000, batch = 5. new adam value .0001 from .01. see if it fixes the pred same prob. 
#t13 v2.17.13 run on hpc. all cells, batch size = 30. ran out of memory and crashed, >20gb used. Reducing batch size, but that may not be hte issue.
#t14 v3.28.22 on synth data 20230802_222843. IT WORKS! YOU NEEDED TO ESTABLISH NODE TYPE!!!! 
synth data used. 
v0.0.1 imported synth_dataset_maker_template_v1.8.3



timestamp = 20230809_175152
genex_num_nodes = 3
genex_num_node_features = 1
genex_mean = 5
genex_std_dev = 1
atac_num_nodes = 3
atac_num_node_features = 1
num_graphs = 10000
target = parabola
len(graph_idx_list) = 10000 

case_count = 5130
control_count = 4870
prop. cases = 0.513
prop. controls = 0.487
causal node to target corr = PearsonRResult(statistic=-0.0031411709547051686, pvalue=0.7534613399352434)
total run time = 29.386375188827515
edge_index = tensor([[0, 1, 2, 0, 1, 2, 3, 4, 5],
        [3, 4, 5, 0, 1, 2, 3, 4, 5]])
first_case_target = tensor([1])
first_case_feature_matrix = tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000],
        [3.7450, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],
        [5.9485, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],
        [5.1642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])
first_control_target = tensor([0])
first_control_feature_matrix = tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000],
        [4.7844, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],
        [2.9712, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],
        [5.4050, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])

#parameters
synth_data = True
synth_data_stamp_name = 20230809_175152 #only meaningful if synth_data = True
hp_run = False
num_graphs = 10000
split_point_for_traintest = 8000
num_epochs = 10
batch_size = 100
pooling_keep_count = 187215
root_dir = synth_datasets/dataset_20230809_175152
learning_rate = 0.01
The model is on cpu. 
 Epoch: 001 | epoch_time: 75.4 | Train Acc: 0.5105 | Train Prec: 0.5105 | Train F1: 0.6759 | Test Acc: 0.5230 | Test Prec: 0.5230 | Test F1: 0.6868 
 Epoch: 002 | epoch_time: 65.0 | Train Acc: 0.5098 | Train Prec: 1.0000 | Train F1: 0.0763 | Test Acc: 0.5000 | Test Prec: 1.0000 | Test F1: 0.0842 
 Epoch: 003 | epoch_time: 66.3 | Train Acc: 0.8246 | Train Prec: 0.7880 | Train F1: 0.8395 | Test Acc: 0.8405 | Test Prec: 0.8104 | Test F1: 0.8561 
 Epoch: 004 | epoch_time: 73.0 | Train Acc: 0.8726 | Train Prec: 0.9154 | Train F1: 0.8689 | Test Acc: 0.8735 | Test Prec: 0.9109 | Test F1: 0.8742 
 Epoch: 005 | epoch_time: 68.7 | Train Acc: 0.8702 | Train Prec: 0.8970 | Train F1: 0.8689 | Test Acc: 0.8785 | Test Prec: 0.9019 | Test F1: 0.8812 
 Epoch: 006 | epoch_time: 69.5 | Train Acc: 0.8514 | Train Prec: 0.8344 | Train F1: 0.8587 | Test Acc: 0.8605 | Test Prec: 0.8458 | Test F1: 0.8705 
 Epoch: 007 | epoch_time: 64.7 | Train Acc: 0.8699 | Train Prec: 0.9161 | Train F1: 0.8655 | Test Acc: 0.8745 | Test Prec: 0.9145 | Test F1: 0.8748 
 Epoch: 008 | epoch_time: 72.0 | Train Acc: 0.8644 | Train Prec: 0.8495 | Train F1: 0.8704 | Test Acc: 0.8620 | Test Prec: 0.8438 | Test F1: 0.8726 
 Epoch: 009 | epoch_time: 66.7 | Train Acc: 0.8728 | Train Prec: 0.9170 | Train F1: 0.8688 | Test Acc: 0.8730 | Test Prec: 0.9116 | Test F1: 0.8735 
 Epoch: 010 | epoch_time: 73.1 | Train Acc: 0.8448 | Train Prec: 0.8297 | Train F1: 0.8520 | Test Acc: 0.8550 | Test Prec: 0.8436 | Test F1: 0.8649timestamp : 20230810_132959