 

#version notes
#v0.2.1 you're going to load only the first 10 of the excitatory cells in the dataset class. 
#v0.3.1 added sag pool, extracted summed the sagpool scores to scoresum, added model save function with timestamp
#v0.4.1 added custom sag pool. See week 12 work notes for how to use. key feature: it returns an extra tensor with the complete sagpool scores.
#problem: pre_drop_score_sum collects scores from all epochs during trainging. We really only want the scores from the last epoch.
#v0.4.2 added total run time measurer
#0.5.2 testing last-epoch only score collector
#0.5.3 changed epoch runtime 
#0.6.3 added the scoresum collector
#0.6.4 fixed scoresum failing when run batch size != set batch_size (ie because num graphs not evenly divisible by batch_size)
#0.6.5 fixed epoch counter bug
#0.7.5 extracted scores with metadata and saved to csv
#0.7.5 fixed tensor size alignment issue with scoresum extraction tensor.size [n] != size [1,n]
#1.7.5 FULL RUN COMPLETE!
#1.8.5 added call rate counter, included it in model results, included weighted score in model results, renamed model results csv. Saved the 
#2.8.5 added two more convolution layers, two normalization layers, a dropout layer, and a second pooling layer. also softmaxxed the attention scores and took their product to output as the total graph attention score. 
#2.9.5 added a notes feature
#2.9.6 fixed misshape feed into sagpool caused by 2x attention heads and 2x hidden layer
#2.10.6 activated all changes from 1.8.5 to 2.9.6.
#2.11.6 added precision and f1 metrics. PROBLEM: IT ALWAYS PREDICTS THE SAME CLASS, sometimes 1, sometimes 0.


#testing notes
#t1: 10 cells loaded in dataset class with 50kb edge index w self loops. 
# Dataset definition bottleneck solved. You shouldn't include raw file names you can't use. 
# Performance is poor: only .4 acc on the test set.
#t2: v0.5.2, 100 cells loaded w 50kb self loop edge index
# runtime: ~5m per epoch, terminated after two epochs
# performance on two epochs: train acc: .6 test acc: .3
#t3: v1.8.5. 600s/epoch, 20ish mins(?) of class instantiation time
#t4: v1.8.5 all excitatory cells, left to run overnight. failed because of memory overallocation
#t5: v1.8.5 tried 20000 excitatory cells. failed because of memory overallocation. always step 1 of forward pass
#t6: v1.8.5 tried 10000 excitatory cells. failed because of memory allocation.
#t7: v1.8.5 tried 5000 excitatory cells. failed mem alloc. I think it's the batch size of 100, not the cell count. 
#t8: v1.8.5 tried 1000 exctatory cells, batch soze = 10. ran fine. total time 14k sec. test acc .63. scores ar weird-- top 1000 all sam score. 
#t9: v1.8.5 tried all excitatory cells, batch size = 10. 
#t10: v2.10.6 tried 1000 excitatory cells, batch size = 10. computer froze.
#t11: v2.10.6 rerun of t10
#t12: v2.11.6 run of 100 cells, batch = 10. need some data for the visualizer. cpu crash. lowering batchs size to 2. 

#parameters
num_graphs = 100
split_point_for_traintest = 80
num_epochs = 10
batch_size = 2
pooling_keep_count = 187215
root_dir = C:\Users\username\OneDrive - Imperial College London\0_Imperial_main_asof_1.19.23\0Thesis Project - Multi-Modal Approach to Single Cell Analysis\single nucleus multiomics identifies... (AD data)\GSE214979 subseries\pts
 
 Epoch: 001 | epoch_time: 546.6 | Train Acc: 0.6250 | Train Prec: 0.6250 | Train F1: 0.7692 | Test Acc: 0.3000 | Test Prec: 0.3000 | Test F1: 0.4615 
 Epoch: 002 | epoch_time: 494.1 | Train Acc: 0.6250 | Train Prec: 0.6250 | Train F1: 0.7692 | Test Acc: 0.3000 | Test Prec: 0.3000 | Test F1: 0.4615 
 Epoch: 003 | epoch_time: 557.4 | Train Acc: 0.6125 | Train Prec: 0.6301 | Train F1: 0.7480 | Test Acc: 0.4000 | Test Prec: 0.3333 | Test F1: 0.5000 
 Epoch: 004 | epoch_time: 539.4 | Train Acc: 0.6250 | Train Prec: 0.6250 | Train F1: 0.7692 | Test Acc: 0.3000 | Test Prec: 0.3000 | Test F1: 0.4615 
 Epoch: 005 | epoch_time: 488.3 | Train Acc: 0.6250 | Train Prec: 0.6250 | Train F1: 0.7692 | Test Acc: 0.3000 | Test Prec: 0.3000 | Test F1: 0.4615 
 Epoch: 006 | epoch_time: 534.8 | Train Acc: 0.6250 | Train Prec: 0.6250 | Train F1: 0.7692 | Test Acc: 0.3000 | Test Prec: 0.3000 | Test F1: 0.4615 
 Epoch: 007 | epoch_time: 545.9 | Train Acc: 0.6250 | Train Prec: 0.6613 | Train F1: 0.7321 | Test Acc: 0.4500 | Test Prec: 0.3529 | Test F1: 0.5217 
 Epoch: 008 | epoch_time: 620.1 | Train Acc: 0.4375 | Train Prec: 0.6000 | Train F1: 0.4000 | Test Acc: 0.5000 | Test Prec: 0.0000 | Test F1: 0.0000 
 Epoch: 009 | epoch_time: 584.4 | Train Acc: 0.6250 | Train Prec: 0.6250 | Train F1: 0.7692 | Test Acc: 0.3000 | Test Prec: 0.3000 | Test F1: 0.4615 
 Epoch: 010 | epoch_time: 609.4 | Train Acc: 0.6250 | Train Prec: 0.6282 | Train F1: 0.7656 | Test Acc: 0.3500 | Test Prec: 0.3158 | Test F1: 0.4800